{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/saved_models/10e_trans_with_relax.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 589\u001b[0m\n\u001b[1;32m    586\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    587\u001b[0m example_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpal 221_3\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[0;32m--> 589\u001b[0m inference \u001b[38;5;241m=\u001b[39m \u001b[43mInferenceWithPatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minform_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgranzyme_b_image_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_data_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m inference\u001b[38;5;241m.\u001b[39mrun(example_sample)\n",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m, in \u001b[0;36mInferenceWithPatch.__init__\u001b[0;34m(self, inform_files, granzyme_b_image_folder, processed_data_folder, model_path, patch_size, manual)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 2 classes: background and Granzyme B\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#self.model.load_state_dict(torch.load(model_path))\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/white_env/lib/python3.12/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/white_env/lib/python3.12/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/white_env/lib/python3.12/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/saved_models/10e_trans_with_relax.pth'"
     ]
    }
   ],
   "source": [
    "import dropbox\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io as skio\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import huggingface_hub\n",
    "from typing import List, Tuple, Dict\n",
    "from accelerate import Accelerator\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from huggingface_hub import upload_file\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.ops import roi_align\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "from random import sample\n",
    "import dropbox\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from skimage import io as skio, img_as_ubyte, measure\n",
    "from skimage.color import rgb2hed, hed2rgb\n",
    "from skimage.exposure import rescale_intensity, equalize_adapthist\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "from cellpose import models\n",
    "import pyclesperanto_prototype as cle\n",
    "from skimage.measure import regionprops\n",
    "import python_calamine\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib.widgets import Button\n",
    "from mpl_interactions import panhandler, zoom_factory\n",
    "import pickle\n",
    "import torchvision.ops as ops\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "'''\n",
    "This code has the vizualization for the relevant cells and the bounding boxes\n",
    "'''\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "ACCESS_TOKEN = os.getenv(\"DROPBOX_ACCESS_TOKEN\")\n",
    "dbx = dropbox.Dropbox(ACCESS_TOKEN)\n",
    "\n",
    "\n",
    "\n",
    "#All the relevant info is in the\n",
    "class InferenceWithPatch:\n",
    "    def __init__(self, inform_files, granzyme_b_image_folder, processed_data_folder, model_path, patch_size, manual=True):\n",
    "        self.inform_files = inform_files\n",
    "        self.granzyme_b_image_folder = granzyme_b_image_folder\n",
    "        self.processed_data_folder = processed_data_folder\n",
    "        self.errors = []\n",
    "        self.model_path = model_path\n",
    "        self.patch_size = patch_size\n",
    "        self.manual = manual\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.model = self.get_model(num_classes=2)  # 2 classes: background and Granzyme B\n",
    "        #self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "    def get_model(self, num_classes):\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "        model.roi_heads.mask_predictor = None\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        return model\n",
    "\n",
    "    def list_files_in_folder(self, folder_path):\n",
    "        files = []\n",
    "        result = dbx.files_list_folder(folder_path)\n",
    "        files.extend([entry.name for entry in result.entries if isinstance(entry, dropbox.files.FileMetadata)])\n",
    "        while result.has_more:\n",
    "            result = dbx.files_list_folder_continue(result.cursor)\n",
    "            files.extend([entry.name for entry in result.entries if isinstance(entry, dropbox.files.FileMetadata)])\n",
    "        return files\n",
    "\n",
    "    def read_image_from_dropbox(self, dropbox_path):\n",
    "        _, res = dbx.files_download(path=dropbox_path)\n",
    "        file_bytes = io.BytesIO(res.content)\n",
    "        image = skio.imread(file_bytes)\n",
    "        return image\n",
    "\n",
    "    def read_excel_from_dropbox(self, dropbox_path):\n",
    "        _, res = dbx.files_download(path=dropbox_path)\n",
    "        file_bytes = io.BytesIO(res.content)\n",
    "        rows = iter(python_calamine.CalamineWorkbook.from_filelike(file_bytes).get_sheet_by_index(0).to_python())\n",
    "        headers = list(map(str, next(rows)))\n",
    "        data = [dict(zip(headers, row)) for row in rows]\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def relevant_rows(self, image_name, inform_excel):\n",
    "        relevant_cells = pd.DataFrame()\n",
    "        for index, row in inform_excel.iterrows():\n",
    "            if image_name in row['Sample Name']:\n",
    "                relevant_cells = relevant_cells._append(row)\n",
    "        return relevant_cells\n",
    "\n",
    "    def global_inform_values(self, inform_df):\n",
    "        mean_nuclei = np.mean(inform_df['Nucleus Autofluorescence Mean (Normalized Counts, Total Weighting)'])\n",
    "        std_nuclei = np.std(inform_df['Nucleus Autofluorescence Mean (Normalized Counts, Total Weighting)'])\n",
    "        auto_98 = mean_nuclei + 2 * std_nuclei\n",
    "        return auto_98\n",
    "\n",
    "    def label_relevant_cells(self, relevant_rows):\n",
    "        cd8_mem = 'Membrane CD8 (Opal 570) Mean (Normalized Counts, Total Weighting)'\n",
    "        cd8_cyt = 'Cytoplasm CD8 (Opal 570) Mean (Normalized Counts, Total Weighting)'\n",
    "        gb_mem = 'Membrane Granzyme B (Opal 650) Mean (Normalized Counts, Total Weighting)'\n",
    "        gb_cyt = 'Cytoplasm Granzyme B (Opal 650) Mean (Normalized Counts, Total Weighting)'\n",
    "        gb_ent = 'Entire Cell Granzyme B (Opal 650) Mean (Normalized Counts, Total Weighting)'\n",
    "        cd8_ent = 'Entire Cell CD8 (Opal 570) Mean (Normalized Counts, Total Weighting)'\n",
    "\n",
    "        relevant_cells = relevant_rows.copy()\n",
    "        relevant_cells.dropna(subset=[cd8_mem, cd8_cyt, gb_mem, gb_cyt, gb_ent], inplace=True)\n",
    "\n",
    "        relevant_cells['Label'] = 'None'\n",
    "\n",
    "        relevant_cells[cd8_mem] = pd.to_numeric(relevant_cells[cd8_mem], errors='coerce')\n",
    "        relevant_cells[cd8_cyt] = pd.to_numeric(relevant_cells[cd8_cyt], errors='coerce')\n",
    "        relevant_cells[gb_mem] = pd.to_numeric(relevant_cells[gb_mem], errors='coerce')\n",
    "        relevant_cells[gb_cyt] = pd.to_numeric(relevant_cells[gb_cyt], errors='coerce')\n",
    "        relevant_cells[gb_ent] = pd.to_numeric(relevant_cells[gb_ent], errors='coerce')\n",
    "        relevant_cells[cd8_ent] = pd.to_numeric(relevant_cells[cd8_ent], errors='coerce')\n",
    "\n",
    "        treshold_cd8 = np.median(relevant_cells[cd8_ent])\n",
    "\n",
    "        pos_phen = ['CD8', 'CD4', 'CD56']\n",
    "        #row['Tissue Category'] == 'Tumor' \n",
    "        not_in_phen = 0\n",
    "        not_gb = 0\n",
    "        gbcy_gbm = 0\n",
    "        gb_cd8 = 0\n",
    "\n",
    "        # Add the labels to the relevant cells dataframe\n",
    "        for index, row in relevant_cells.iterrows():\n",
    "            if row['Phenotype'] in pos_phen:\n",
    "                if row[gb_ent] > 0 and row[gb_cyt] > 0:\n",
    "                    #if row[aut_nuclei] > auto_98:\n",
    "                    if treshold_cd8 >= row[cd8_ent]:\n",
    "                        relevant_cells.at[index, 'Label'] = 'gb'\n",
    "                    else:\n",
    "                        gb_cd8 += 1\n",
    "                        relevant_cells.at[index, 'Label'] = 'gb_cd8'\n",
    "                else:\n",
    "                    not_gb += 1\n",
    "                    relevant_cells.at[index, 'Label'] = 'not_gb'\n",
    "            else:\n",
    "                not_in_phen += 1\n",
    "                relevant_cells.at[index, 'Label'] = 'not_phen'\n",
    "\n",
    "        print('Number of cells not in phen:', not_in_phen)\n",
    "        print('Number of cells not gb:', not_gb)\n",
    "        print('Number of cells gbvy_gbm:', gbcy_gbm)\n",
    "        print('Number of cells gbct_cd8:', gb_cd8)\n",
    "\n",
    "        positive_gb_cells = relevant_cells[relevant_cells['Label'] == 'gb']\n",
    "        #not_phen_cells = relevant_cells[relevant_cells['Label'] == 'not_phen']\n",
    "        #not_gb_cells = relevant_cells[relevant_cells['Label'] == 'not_gb']\n",
    "        #gbcy_gbm_cells = relevant_cells[relevant_cells['Label'] == 'gbcy_gbm']\n",
    "        #gbct_cd8_cells = relevant_cells[relevant_cells['Label'] == 'gbct_cd8']\n",
    "\n",
    "        print('Number of positive Granzyme B cells:', len(positive_gb_cells))\n",
    "        print('Number of  cells:', len(relevant_cells))\n",
    "        return relevant_cells\n",
    "\n",
    "\n",
    "    def create_patches_no_box(self, image, patch_size):\n",
    "        patches = []\n",
    "        img_height, img_width = image.shape[:2]\n",
    "        for i in range(0, img_height, patch_size):\n",
    "            for j in range(0, img_width, patch_size):\n",
    "                patch = image[i:i + patch_size, j:j + patch_size]\n",
    "                patches.append((patch, i, j))\n",
    "        return patches\n",
    "\n",
    "    def run_inference_on_patches(self, patches):\n",
    "        updated_patches = []\n",
    "        self.model.eval()\n",
    "        for patch, i, j in patches:\n",
    "            patch_tensor = F.to_tensor(patch).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                prediction = self.model(patch_tensor)[0]\n",
    "            pred_boxes = prediction['boxes'].cpu().numpy()\n",
    "            pred_scores = prediction['scores'].cpu().numpy()\n",
    "            updated_patches.append((patch, pred_boxes, pred_scores, i, j))\n",
    "        return updated_patches\n",
    "\n",
    "\n",
    "    def generate_bounding_boxes(self, patch, criteria_labels, pred_boxes, pred_scores,i_offset, j_offset):\n",
    "        \n",
    "        boxes = []\n",
    "        print('Starting processing...')\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(patch)\n",
    "        relevant_rows = criteria_labels.copy()\n",
    "\n",
    "        margin = 0  # Large margin to accommodate the possible bounding boxes\n",
    "\n",
    "        score_treshold = 0 # Has a low confidence threshold eg 0.5 to avoid false negatives and allow for manual elimination\n",
    "\n",
    "        pred_boxes = np.array(pred_boxes)\n",
    "        pred_scores = np.array(pred_scores)\n",
    "\n",
    "        sorted_indices = np.argsort(pred_scores)[::-1]\n",
    "\n",
    "        sorted_pred_boxes = pred_boxes[sorted_indices]\n",
    "        sorted_pred_scores = pred_scores[sorted_indices]\n",
    "\n",
    "        done_point = []\n",
    "\n",
    "        # adjust relevant rows so we only look at the cells in the patch\n",
    "        relevant_rows = relevant_rows.copy()\n",
    "        x_max_patch = j_offset + patch.shape[1]\n",
    "        y_max_patch = i_offset + patch.shape[0]\n",
    "        x_min_patch = j_offset\n",
    "        y_min_patch = i_offset\n",
    "        relevant_rows = relevant_rows[(relevant_rows['Cell X Position'] >= x_min_patch) & (relevant_rows['Cell X Position'] <= x_max_patch)]\n",
    "        relevant_rows = relevant_rows[(relevant_rows['Cell Y Position'] >= y_min_patch) & (relevant_rows['Cell Y Position'] <= y_max_patch)]\n",
    "\n",
    "        \n",
    "        for index, row in relevant_rows.iterrows():\n",
    "            x0 = row['Cell X Position']\n",
    "            y0 = row['Cell Y Position']\n",
    "            if row['Label'] == 'gb':\n",
    "                ax.plot(x0 - j_offset, y0 - i_offset, 'o', color='green', markersize=4)\n",
    "                #ax.text(x0 - j_offset, y0 - i_offset - 3, 'gb', color='black', fontsize=8, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            elif row['Label'] == 'not_gb':\n",
    "                ax.plot(x0 - j_offset, y0 - i_offset, 'o', color='blue', markersize=4)\n",
    "               # ax.text(x0 - j_offset, y0 - i_offset - 3, 'no gb', color='black', fontsize=8, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            elif row['Label'] == 'gbcy_gbm':\n",
    "                ax.plot(x0 - j_offset, y0 - i_offset, 'o', color='black', markersize=4)\n",
    "                #ax.text(x0 - j_offset, y0 - i_offset - 3, 'gb_cytop < gb_mem', color='black', fontsize=8, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            elif row['Label'] == 'gbct_cd8':\n",
    "                ax.plot(x0 - j_offset, y0 - i_offset, 'o', color='purple', markersize=4)\n",
    "                #ax.text(x0 - j_offset, y0 - i_offset - 3, 'gb cyto < cd8 cyto', color='black', fontsize=8, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            elif row['Label'] == 'not_phen':\n",
    "                ax.plot(x0 - j_offset, y0 - i_offset, 'o', color='red', markersize=4)\n",
    "                #ax.text(x0 - j_offset, y0 - i_offset - 3, 'not phenotype', color='black', fontsize=8, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "           \n",
    "        for pred_box, score in zip(sorted_pred_boxes, sorted_pred_scores):\n",
    "            if score < score_treshold:\n",
    "                continue\n",
    "            x_min, y_min, x_max, y_max = pred_box\n",
    "            rect = mpatches.Rectangle(\n",
    "                (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                linewidth=2, edgecolor='blue', facecolor='none', linestyle='dashed'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "\n",
    "            expanded_minc = max(0, x_min - margin) + j_offset\n",
    "            expanded_maxc = x_max + margin + j_offset\n",
    "            expanded_minr = max(0, y_min - margin) + i_offset\n",
    "            expanded_maxr = y_max + margin + i_offset\n",
    "\n",
    "            rel = relevant_rows[(relevant_rows['Label'] == 'gb')]\n",
    "\n",
    "            for _, row in rel.iterrows():\n",
    "                x_position = row['Cell X Position']\n",
    "                y_position = row['Cell Y Position']\n",
    "\n",
    "                if (x_position, y_position) in done_point:\n",
    "                    continue\n",
    "\n",
    "                if expanded_minc <= x_position <= expanded_maxc and expanded_minr <= y_position <= expanded_maxr:\n",
    "                    #print(\"yess\")\n",
    "                    #ax.plot(x_position - j_offset, y_position - i_offset, 'o', color='purple', markersize=4)\n",
    "                    #ph = row['Phenotype']\n",
    "                    #ax.text(x_position - j_offset, y_position - i_offset - 3, ph, color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "                    done_point.append((x_position, y_position))\n",
    "\n",
    "\n",
    "                    boxes.append({\n",
    "                        'Cell ID': row['Cell ID'],\n",
    "                        'X Position': int(x_position),\n",
    "                        'Y Position': int(y_position),\n",
    "                        'Bounding Box': [int(x_min + j_offset), int(y_min + i_offset), int(x_max+ j_offset), int(y_max + i_offset)],\n",
    "                        'Granzyme B': row['Entire Cell Granzyme B (Opal 650) Mean (Normalized Counts, Total Weighting)'],\n",
    "                        'Score': score\n",
    "                    })\n",
    "                    # Plots the patches \n",
    "                    rect = mpatches.Rectangle(\n",
    "                        (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                        linewidth=2, edgecolor='red', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    ax.text(x_min, y_min - 10, f'{score:.2f}', color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "                    break\n",
    "        \n",
    "        custom_lines = [\n",
    "            Line2D([0], [0], color='green', marker='o', linestyle='None', markersize=6, label='Positive GB'),\n",
    "            Line2D([0], [0], color='blue', marker='o', linestyle='None', markersize=6, label='No Granzyme B'),\n",
    "            Line2D([0], [0], color='black', marker='o', linestyle='None', markersize=6, label='GB cyto < GB mem'),\n",
    "            Line2D([0], [0], color='purple', marker='o', linestyle='None', markersize=6, label='GB cyto < CD8 cyto'),\n",
    "            Line2D([0], [0], color='red', marker='o', linestyle='None', markersize=6, label='Not relevant phenotype'),\n",
    "            mpatches.Patch(edgecolor='blue', linestyle='dashed', fill=False, label='Model predicted Box'),\n",
    "            mpatches.Patch(edgecolor='red', linestyle='solid', fill=False, label= 'Refined Box')\n",
    "        ]\n",
    "        # make the legend solid white, i.e., facecolor='white' and alpha=1\n",
    "        #leg = ax.legend(handles=custom_lines, loc='upper right')\n",
    "        #leg.get_frame().set_facecolor('white')\n",
    "        #leg.get_frame().set_alpha(1)\n",
    "\n",
    "        plt.axis('off')\n",
    "        #plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return boxes\n",
    "\n",
    "\n",
    "    def reconstruct_image(self, original_image, patches_with_boxes, patch_size):\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.imshow(original_image)\n",
    "        for patch, patch_boxes, i, j in patches_with_boxes:\n",
    "            for patch_box in patch_boxes:\n",
    "                bb = patch_box[\"Bounding Box\"]\n",
    "                x_min, y_min, x_max, y_max = bb\n",
    "                rect = mpatches.Rectangle(\n",
    "                    #(x_min , y_min), (x_max-j) - (x_min-j), (y_max-i) - (y_min-i),\n",
    "                    (x_min, y_min ), x_max - x_min, y_max - y_min,\n",
    "                    linewidth=2, edgecolor='red', facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x_min, y_min, f'{patch_box[\"Score\"]:.2f}', color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            original_image[i:i + patch_size, j:j + patch_size] = patch\n",
    "        return original_image\n",
    "\n",
    "    def plot_image_with_boxes(self, image, boxes, pred_boxes=None, pred_scores=None, title=\"Image with Bounding Boxes\"):\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.imshow(image)\n",
    "        for box in boxes:\n",
    "            bb = box[\"Bounding Box\"]\n",
    "            rect = mpatches.Rectangle(\n",
    "                (bb[0], bb[1]), bb[2] - bb[0], bb[3] - bb[1],\n",
    "                linewidth=2, edgecolor='red', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(bb[0], bb[1] - 20, f'{box[\"Score\"]:.2f}', color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "        if pred_boxes is not None:\n",
    "            for pred_box, score in zip(pred_boxes, pred_scores):\n",
    "                rect = mpatches.Rectangle(\n",
    "                    (pred_box[0], pred_box[1]), pred_box[2] - pred_box[0], pred_box[3] - pred_box[1],\n",
    "                    linewidth=2, edgecolor='blue', facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(pred_box[0], pred_box[1] - 10, f'{score:.2f}', color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # if you return fig can you plot later\n",
    "\n",
    "    def manual_elimination(self, image, boxes):\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "        ax.imshow(image)\n",
    "        patches = []\n",
    "        texts = []\n",
    "        selected = [False] * len(boxes)  # Track selection status of each box\n",
    "\n",
    "        for box in boxes:\n",
    "            bb = box[\"Bounding Box\"]\n",
    "            rect = mpatches.Rectangle(\n",
    "                (bb[0], bb[1]), bb[2] - bb[0], bb[3] - bb[1],\n",
    "                linewidth=2, edgecolor='blue', facecolor='none'\n",
    "            )\n",
    "            patches.append(rect)\n",
    "            ax.add_patch(rect)\n",
    "            text = ax.text(bb[0], bb[1] - 20, f'{box[\"Score\"]:.2f}', color='black', fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "            texts.append(text)\n",
    "        plt.title('Manual Elimination of Bounding Boxes')\n",
    "\n",
    "        def on_click(event):\n",
    "            if event.inaxes != ax:\n",
    "                return\n",
    "            for i, (box, rect, text) in enumerate(zip(boxes, patches, texts)):\n",
    "                bb = box[\"Bounding Box\"]\n",
    "                x_min, y_min, x_max, y_max = bb\n",
    "                if x_min < event.xdata < x_max and y_min < event.ydata < y_max:\n",
    "                    selected[i] = not selected[i]\n",
    "                    if selected[i]:\n",
    "                        rect.set_edgecolor('red')\n",
    "                        rect.set_linestyle('dotted')\n",
    "                        text.set_visible(False)\n",
    "                    else:\n",
    "                        rect.set_edgecolor('blue')\n",
    "                        rect.set_linestyle('solid')\n",
    "                        text.set_visible(True)\n",
    "                    fig.canvas.draw()\n",
    "\n",
    "        fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "\n",
    "        def on_done(event):\n",
    "            plt.close(fig)\n",
    "\n",
    "        ax_button = plt.axes([0.9, 0.0, 0.1, 0.075])\n",
    "        button = Button(ax_button, 'Done')\n",
    "        button.on_clicked(on_done)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Enable smooth zoom and pan functionality\n",
    "        zoom_factory(ax)\n",
    "        panhandler(fig)\n",
    "        fig.canvas.manager.toolbar.zoom()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Update boxes list based on selected status\n",
    "        updated_boxes = [box for i, box in enumerate(boxes) if not selected[i]]\n",
    "\n",
    "        return updated_boxes\n",
    "    \n",
    "\n",
    "    def full_processing(self, image, patch_size, criteria_label):\n",
    "        patches = self.create_patches_no_box(image, patch_size)\n",
    "        patches = self.run_inference_on_patches(patches)\n",
    "        patches_with_boxes = []\n",
    "\n",
    "        for patch, pred_boxes, pred_scores, i, j in patches:\n",
    "            #properties = self.process_image(patch)\n",
    "            properties = None\n",
    "            print('Generating bounding boxes...')\n",
    "            boxes = self.generate_bounding_boxes(patch, criteria_label, pred_boxes, pred_scores, i, j)\n",
    "            patches_with_boxes.append((patch, boxes, i, j))\n",
    "\n",
    "        all_boxes = [box for _, boxes, _, _ in patches_with_boxes for box in boxes]\n",
    "        print('Number of boxes:', len(all_boxes))\n",
    "        print('Type of all_boxes:', type(all_boxes))\n",
    "\n",
    "        if self.manual:\n",
    "            valid_boxes = self.manual_elimination(image, all_boxes)\n",
    "            exlude_boxes = [box for box in all_boxes if box not in valid_boxes]\n",
    "            return valid_boxes\n",
    "\n",
    "        #new_image = self.reconstruct_image(image, patches_with_boxes, patch_size)\n",
    "        #self.plot_image_with_boxes(new_image, all_boxes, title=\"Reconstructed Image with Bounding Boxes\")\n",
    "        return all_boxes\n",
    "\n",
    "    def color_separate(self, ihc_rgb):\n",
    "        ihc_hed = rgb2hed(ihc_rgb)\n",
    "        null = np.zeros_like(ihc_hed[:, :, 0])\n",
    "        ihc_h = img_as_ubyte(hed2rgb(np.stack((ihc_hed[:, :, 0], null, null), axis=-1)))\n",
    "        ihc_e = img_as_ubyte(hed2rgb(np.stack((null, ihc_hed[:, :, 1], null), axis=-1)))\n",
    "        ihc_d = img_as_ubyte(hed2rgb(np.stack((null, null, ihc_hed[:, :, 2]), axis=-1)))\n",
    "        h = rescale_intensity(ihc_hed[:, :, 0], out_range=(0, 1), in_range=(0, np.percentile(ihc_hed[:, :, 0], 99)))\n",
    "        d = rescale_intensity(ihc_hed[:, :, 2], out_range=(0, 1), in_range=(0, np.percentile(ihc_hed[:, :, 2], 99)))\n",
    "        zdh = img_as_ubyte(np.dstack((null, d, h)))\n",
    "        return ihc_h, ihc_e, ihc_d, zdh\n",
    "\n",
    "    def process_image(self, gb):\n",
    "        H, _, D, _ = self.color_separate(gb)\n",
    "        hematoxylin_eq = equalize_adapthist(H[:, :, 0])\n",
    "        input_image = 1 - hematoxylin_eq\n",
    "\n",
    "        model = models.Cellpose(gpu=True, model_type='nuclei')\n",
    "        masks, flows, styles, diams = model.eval(input_image, diameter=None, channels=[0, 0])\n",
    "\n",
    "        segmented_np = masks\n",
    "        #plt.imshow(segmented_np)\n",
    "\n",
    "        properties = measure.regionprops(segmented_np, intensity_image=H[:, :, 0])\n",
    "        # print number of cells\n",
    "\n",
    "   \n",
    "\n",
    "        #ax.axis('off')\n",
    "        #plt.show()\n",
    "        return properties\n",
    "\n",
    "    def save_processed_data(self, sample_name, image_name, data):\n",
    "        sample_folder = os.path.join(self.processed_data_folder, sample_name)\n",
    "        if not os.path.exists(sample_folder):\n",
    "            os.makedirs(sample_folder)\n",
    "        file_path = os.path.join(sample_folder, f'{image_name}.pkl')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "\n",
    "    def load_processed_data(self, sample_name, image_name):\n",
    "        file_path = os.path.join(self.processed_data_folder, sample_name, f'{image_name}.pkl')\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        return None\n",
    "    \n",
    "    def plot_image_with_inform(self, image, inform_excel):\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.imshow(image)\n",
    "\n",
    "        #pos_phen = ['CD8', 'CD4', 'CD56']\n",
    "        for index, row in inform_excel.iterrows():\n",
    "            x_position = row['Cell X Position']\n",
    "            y_position = row['Cell Y Position']\n",
    "            ax.plot(x_position, y_position, 'o', color='red', markersize=2)\n",
    "\n",
    "        ax.plot(0, 0, 'o', color='green', markersize=6)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def run(self, example):\n",
    "        inform_files = self.list_files_in_folder(self.inform_files)\n",
    "        image_files = self.list_files_in_folder(self.granzyme_b_image_folder)\n",
    "        \n",
    "        for inform_file in inform_files:\n",
    "            if inform_file.endswith('.xlsx'):\n",
    "                inform_path = self.inform_files + inform_file\n",
    "                inform_excel = self.read_excel_from_dropbox(inform_path)\n",
    "                name_sample = inform_file.split('.xlsx')[0]\n",
    "                print('Starting comparison')\n",
    "                if name_sample != example:\n",
    "                    continue\n",
    "\n",
    "                print(f'Working on sample: {name_sample}')\n",
    "                #phenotypes = inform_excel['Phenotype'].unique()\n",
    "                #print(f'Phenotypes in sample: {phenotypes}')\n",
    "                #tissue_types = inform_excel['Tissue Category'].unique()\n",
    "                #print(f'Tissue types in sample: {tissue_types}')\n",
    "                relevant_images = [f for f in image_files if name_sample in f and 'Granzyme' in f]\n",
    "                print(f'Number of images in sample: {len(relevant_images)}')\n",
    "                #max_x_pos = inform_excel['Cell X Position'].max()\n",
    "                #max_y_pos = inform_excel['Cell Y Position'].max()\n",
    "                #print(f'Max X Position: {max_x_pos}')\n",
    "                #print(f'Max Y Position: {max_y_pos}')\n",
    "                #min_x_pos = inform_excel['Cell X Position'].min()\n",
    "                #min_y_pos = inform_excel['Cell Y Position'].min()\n",
    "                #print(f'Min X Position: {min_x_pos}')\n",
    "\n",
    "                for image_file in relevant_images:\n",
    "                    image_name = image_file.split('_Granzyme')[0]\n",
    "                    try:\n",
    "                        image_path = self.granzyme_b_image_folder + image_file\n",
    "                        print(f'Processing image: {image_name}')\n",
    "                        relevant_rows = self.relevant_rows(image_name, inform_excel)\n",
    "                        criteria_labels= self.label_relevant_cells(relevant_rows)\n",
    "                        gb_image = self.read_image_from_dropbox(image_path)\n",
    "                        print('image shape:', gb_image.shape)\n",
    "                        #self.plot_image_with_inform(gb_image, relevant_rows)\n",
    "                        all_boxes = self.full_processing(gb_image, self.patch_size, criteria_labels)\n",
    "                    except Exception as e:\n",
    "                        print(f'Error processing {image_file}: {e}')\n",
    " \n",
    "\n",
    "                print(f'Done processing sample: {name_sample}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    inform_files = '/Rebeca&Laura/inform_in_excel/'\n",
    "    granzyme_b_image_folder = '/UEC, CD8 and GranzymeB/test/'\n",
    "    processed_data_folder = 'data/saved_models/10e_trans_with_relax.pth'\n",
    "    model_path =  'data/saved_models/10e_trans_with_relax.pth'\n",
    "    #data/saved_models/20e_trans_with_relax.pth\n",
    "    #data/saved_models/sextra_impie_then_manual.pth copy\n",
    "    patch_size = 256\n",
    "    example_sample = 'Opal 221_3'  # Example\n",
    "\n",
    "    inference = InferenceWithPatch(inform_files, granzyme_b_image_folder, processed_data_folder, model_path, patch_size)\n",
    "    inference.run(example_sample)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "white_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
